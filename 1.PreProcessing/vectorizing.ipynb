{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0286a73",
   "metadata": {},
   "source": [
    "# Vectorizing the NL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64138ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/shreyanakum/Documents/CS178/CS-178-Project/.venv/lib/python3.12/site-packages (from scikit-learn) (2.3.5)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/shreyanakum/Documents/CS178/CS-178-Project/.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0\n",
      "Requirement already satisfied: pandas in /Users/shreyanakum/Documents/CS178/CS-178-Project/.venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/shreyanakum/Documents/CS178/CS-178-Project/.venv/lib/python3.12/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/shreyanakum/Documents/CS178/CS-178-Project/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shreyanakum/Documents/CS178/CS-178-Project/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/shreyanakum/Documents/CS178/CS-178-Project/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shreyanakum/Documents/CS178/CS-178-Project/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U scikit-learn\n",
    "! pip install pandas\n",
    "! pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b6493f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e518a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DATA/train_updated.csv\")\n",
    "label_cols = df.columns\n",
    "label_cols = label_cols[3:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a936b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text_clean'], \n",
    "    df[label_cols],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93faf4f",
   "metadata": {},
   "source": [
    "## Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b0bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorization:\n",
      "Training shape: (127656, 5000)\n",
      "Testing shape: (31915, 5000)\n",
      "Vocab size: 5000\n"
     ]
    }
   ],
   "source": [
    "## Vectorizers: https://mljourney.com/tf-idf-vectorizer-vs-countvectorizer-the-key-differences-for-text-analysis/\n",
    "# We can use both of these and compare how models do depending on the vectorizer if we want to.\n",
    "# TF-IDF Vectorization (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "# This is better rep of the data but is usually more complicated\n",
    "print(\"TF-IDF Vectorization:\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2), # unigrams and bigrams\n",
    "    min_df=5, # ignore terms that appear in less than 5\n",
    "    max_df=0.8 # ignore terms that appear in more than 80%\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"Training shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Testing shape: {X_test_tfidf.shape}\")\n",
    "print(f\"Vocab size: {len(tfidf.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edab6ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorization\n",
      "Training shape: (127656, 5000)\n",
      "Testing shape: (31915, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorization (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "# Much simpler vectorizer since it's based purely on counts\n",
    "print(\"Count Vectorization\")\n",
    "count_vec = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "X_train_count = count_vec.fit_transform(X_train)\n",
    "X_test_count = count_vec.transform(X_test)\n",
    "\n",
    "print(f\"Training shape: {X_train_count.shape}\")\n",
    "print(f\"Testing shape: {X_test_count.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f303329",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c012cb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 words in toxic comments:\n",
      "         word  toxic_score\n",
      "32       fuck     0.120268\n",
      "45       like     0.057798\n",
      "24        don     0.050216\n",
      "96  wikipedia     0.049664\n",
      "42       just     0.048543\n",
      "75       stop     0.040768\n",
      "55       page     0.036231\n",
      "57     people     0.035855\n",
      "43       know     0.035074\n",
      "83      think     0.026563\n",
      "92       want     0.023648\n",
      "19        did     0.022887\n",
      "3     article     0.022641\n",
      "78       talk     0.022412\n",
      "9       block     0.022211\n",
      "Top 15 words in non-toxic comments:\n",
      "         word  non_toxic_score\n",
      "3     article         0.075722\n",
      "78       talk         0.064025\n",
      "55       page         0.057674\n",
      "96  wikipedia         0.052549\n",
      "42       just         0.043774\n",
      "45       like         0.039543\n",
      "24        don         0.035662\n",
      "83      think         0.034837\n",
      "80     thanks         0.033272\n",
      "43       know         0.031585\n",
      "25       edit         0.029388\n",
      "57     people         0.027950\n",
      "19        did         0.027721\n",
      "84       time         0.026802\n",
      "4    articles         0.024652\n"
     ]
    }
   ],
   "source": [
    "# Get top words for toxic comments\n",
    "toxic_texts = df[df['toxic'] == 1]['text_clean']\n",
    "non_toxic_texts = df[df['toxic'] == 0]['text_clean']\n",
    "\n",
    "# Fit on toxic vs non-toxic\n",
    "tfidf_analysis = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "tfidf_analysis.fit(pd.concat([toxic_texts, non_toxic_texts]))\n",
    "\n",
    "toxic_vectors = tfidf_analysis.transform(toxic_texts)\n",
    "non_toxic_vectors = tfidf_analysis.transform(non_toxic_texts)\n",
    "\n",
    "# Get mean TF-IDF scores\n",
    "toxic_means = np.array(toxic_vectors.mean(axis=0)).flatten()\n",
    "non_toxic_means = np.array(non_toxic_vectors.mean(axis=0)).flatten()\n",
    "\n",
    "feature_names = tfidf_analysis.get_feature_names_out()\n",
    "toxic_importance = pd.DataFrame({\n",
    "    'word': feature_names,\n",
    "    'toxic_score': toxic_means,\n",
    "    'non_toxic_score': non_toxic_means,\n",
    "    'difference': toxic_means - non_toxic_means\n",
    "})\n",
    "\n",
    "print(\"Top 15 words in toxic comments:\")\n",
    "print(toxic_importance.nlargest(15, 'toxic_score')[['word', 'toxic_score']])\n",
    "\n",
    "print(\"Top 15 words in non-toxic comments:\")\n",
    "print(toxic_importance.nlargest(15, 'non_toxic_score')[['word', 'non_toxic_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b4a63",
   "metadata": {},
   "source": [
    "## Save Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to-do"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
